{
  "items": [
    {
      "id": "context-reorganization-kumar-2025",
      "name": "On the Reorganization of Representations from Pretrained to In-Context",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "LLMs reorganize their representations from pretrained semantics to context-specified structures when given sufficient in-context examples",
      "publicationDate": "January 2025",
      "authors": [
        "Akarsh Kumar",
        "Dongchen Han",
        "et al."
      ],
      "abstract": "Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy \"graph tracing\" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.",
      "relevanceToProject": "Foundational work from our team showing LLMs represent data generating processes in context - key evidence that internal representations can be dynamically reorganized",
      "readingStatus": "read",
      "link": "https://arxiv.org/abs/2501.00070"
    },
    {
      "id": "td-errors-llms-demircan-2024",
      "name": "Large Language Models Can Solve Reinforcement Learning Problems In-Context Using Temporal Difference Errors",
      "type": "paper",
      "tab": "literature",
      "subtab": "core",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "LLMs develop representations matching temporal difference errors when solving RL problems in-context, despite only being trained for next-token prediction",
      "publicationDate": "October 2024",
      "authors": [
        "Safa Demircan",
        "Canfer Akbulut",
        "Tankred Saanum",
        "et al."
      ],
      "abstract": "In-context learning, the ability to adapt based on a few examples in the input prompt, is a ubiquitous feature of large language models (LLMs). However, as LLMs' in-context learning abilities continue to improve, understanding this phenomenon mechanistically becomes increasingly important. In particular, it is not well-understood how LLMs learn to solve specific classes of problems, such as reinforcement learning (RL) problems, in-context. Through three different tasks, we first show that Llama $3$ $70$B can solve simple RL problems in-context. We then analyze the residual stream of Llama using Sparse Autoencoders (SAEs) and find representations that closely match temporal difference (TD) errors. Notably, these representations emerge despite the model only being trained to predict the next token. We verify that these representations are indeed causally involved in the computation of TD errors and $Q$-values by performing carefully designed interventions on them. Taken together, our work establishes a methodology for studying and manipulating in-context learning with SAEs, paving the way for a more mechanistic understanding.",
      "relevanceToProject": "Direct evidence of error signals in transformers - the closest existing work to finding reward-like representations in LLMs",
      "readingStatus": "to-read",
      "priority": 10,
      "link": "https://arxiv.org/abs/2410.01280"
    },
    {
      "id": "prime-process-reinforcement-2025",
      "name": "PRIME: Process Reinforcement through Implicit Rewards",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "RL method using implicit process rewards derived from policy rollouts and outcome labels, avoiding explicit reward model training",
      "publicationDate": "January 2025",
      "authors": [
        "Zhaohan Daniel Guo",
        "Mohammad Gheshlaghi Azar",
        "Bilal Piot",
        "et al."
      ],
      "abstract": "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implicit process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitive math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.",
      "relevanceToProject": "Explores implicit process rewards in LLMs - relevant for understanding how reward signals might emerge without explicit training",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2502.01456"
    },
    {
      "id": "icrl-survey-2025",
      "name": "In-Context Reinforcement Learning: A Survey",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Survey of agents that solve new RL tasks by conditioning on context without updating network parameters",
      "publicationDate": "February 2025",
      "authors": [
        "The authors"
      ],
      "abstract": "Reinforcement learning (RL) agents typically optimize their policies by performing expensive backward passes to update their network parameters. However, some agents can solve new tasks without updating any parameters by simply conditioning on additional context such as their action-observation histories. This paper surveys work on such behavior, known as in-context reinforcement learning.",
      "relevanceToProject": "Comprehensive survey of in-context RL - important for understanding the landscape of reward-based learning without parameter updates",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2502.07978"
    },
    {
      "id": "algorithm-distillation-2022",
      "name": "Algorithm Distillation: Learning to Reinforcement Learn by Modeling Training Histories",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Distills RL algorithms into neural networks by training transformers on learning histories to perform in-context RL",
      "publicationDate": "October 2022",
      "authors": [
        "Michael Laskin",
        "Luyu Wang",
        "Junhyuk Oh",
        "et al."
      ],
      "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.",
      "relevanceToProject": "Foundational work on in-context RL showing transformers can learn RL algorithms through training histories",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2210.14215"
    },
    {
      "id": "icrl-contextual-bandit-2024",
      "name": "In-Context Reinforcement Learning with Contextual Bandits",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "LLMs demonstrate in-context RL in contextual bandit settings, learning online from external rewards rather than supervised data",
      "publicationDate": "October 2024",
      "authors": [
        "Ofir Nabati",
        "Tom Zahavy",
        "Shie Mannor"
      ],
      "abstract": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.",
      "relevanceToProject": "Shows LLMs can learn from reward signals in-context, relevant for understanding reward processing mechanisms",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2410.05362"
    },
    {
      "id": "icrl-prompting-2024",
      "name": "In-Context Reinforcement Learning Emerges in LLM Inference Time",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Multi-round prompting framework showing LLMs can maximize scalar reward signals at inference time like RL algorithms",
      "publicationDate": "June 2024",
      "authors": [
        "Jiaru Zou",
        "Mengxi Liu",
        "Xingyu Zhao",
        "et al."
      ],
      "abstract": "Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.",
      "relevanceToProject": "Demonstrates emergent RL behavior at inference time - evidence that reward processing might be intrinsic to LLMs",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2506.06303"
    },
    {
      "id": "belief-state-geometry-transformers-2024",
      "name": "Transformers represent belief state geometry in their residual stream",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Transformers linearly represent belief states over hidden data-generating processes in their residual streams, even for fractal geometries",
      "publicationDate": "May 2024",
      "authors": [
        "Adam S. Shai",
        "Sarah E. Marzen",
        "Lucas Teixeira",
        "Alexander Gietelink Oldenziel",
        "Paul M. Riechers"
      ],
      "abstract": "What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a general framework connecting the structure of training data to the geometric structure of activations inside transformers.",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2405.15943"
    }
  ]
}