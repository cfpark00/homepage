{
  "items": [
    {
      "id": "next-token-to-mathematics-2024",
      "name": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models",
      "type": "paper",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Shows that mathematical skills emerge in LLMs following human curriculum order despite random training data presentation",
      "publicationDate": "July 2024",
      "authors": [
        "Shubhra Mishra",
        "Gabriel Poesia",
        "Noah D. Goodman"
      ],
      "abstract": "Large Language Models (LLMs) solely trained on next-token prediction learn to solve a wide range of problems involving mathematical reasoning. But how does this ability evolve during training? We show the first analysis of how mathematical reasoning abilities of several open-weight LLMs develop during pre-training and post-training. To this end, we construct MathCAMPS, a synthetic dataset of novel mathematical reasoning problems grounded in 44 fine-grained skills taken from the Common Core curriculum from K to 8th grades. In one experiment, we show that mathematical skills are learned during pre-training in an order that measurably correlates with the human-designed curriculum, even though training data are randomly ordered. We also show a detailed analysis of which mathematical abilities benefit from instruction tuning, a widely used post-training method and, in contrast, which skills suffer. Our work paves the way for an empirical understanding of LLM training dynamics in relation to reasoning.",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2407.00900"
    },
    {
      "id": "effects-excluding-domains-llm-pretraining",
      "name": "Effects of Excluding Domains in Language Model Pretraining",
      "type": "document",
      "tab": "literature",
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Review showing that excluding training domains causes proportional capability gaps, with broad domains (web, books) having outsized impact on general knowledge",
      "link": "https://docs.google.com/document/d/1P2Nik0m1tLoMx7YZ2Ac4xBnihm0fB_S4niyWHCnf2iY/edit?usp=sharing",
      "abstract": "This document reviews domain ablation experiments in language model pretraining, examining how deliberately omitting specific domains (e.g., history, physics, biology) from training data affects model capabilities. The research synthesizes findings from multiple studies including Longpre et al. (2023) who trained 28 models on Pile dataset variants with systematically removed domains, Rae et al. (2022) from DeepMind's Gopher project who experimented with domain weighting schemes, and domain-specific models like BioBERT. Key findings demonstrate that: (1) Omitting a domain directly degrades in-domain performance - models without biomedical training struggle on biomedical QA, (2) Broad heterogeneous domains (web crawl, books) have disproportionate impact - their removal causes larger performance drops across all tasks due to their diverse knowledge coverage, (3) Comprehensive multi-domain training yields the most robust models - best performance comes from full domain mixtures rather than single-domain or pruned datasets, (4) Domain-specific models like BioBERT and SciBERT outperform general models on specialized tasks precisely because base models lack sufficient domain exposure, and (5) Optimal domain mixing can improve performance across all domains simultaneously, as shown by the DoReMi technique. The research confirms that 'you get what you train for' - models are only as knowledgeable as their training data, with modern LLMs requiring expansive, diverse datasets covering code, books, web content, and academic texts to achieve broad capabilities.",
      "readingStatus": "to-read"
    },
    {
      "id": "gsm-symbolic-2024",
      "name": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
      "type": "paper",
      "tab": "literature",
      "tags": ["Motivation"],
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Reveals significant limitations in LLMs' mathematical reasoning by introducing a new benchmark that exposes their fragility in logical problem-solving",
      "publicationDate": "October 2024",
      "authors": [
        "Iman Mirzadeh",
        "Keivan Alizadeh",
        "Hooman Shahrokhi",
        "Oncel Tuzel",
        "Samy Bengio",
        "Mehrdad Farajtabar"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2410.05229"
    },
    {
      "id": "pretrainers-guide-2023",
      "name": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
      "type": "paper",
      "tab": "literature",
      "tags": ["Related Work"],
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Systematic study of pretraining data effects, showing trade-offs between quality filtering and toxicity, temporal degradation, and benefits of heterogeneous data sources",
      "publicationDate": "May 2023",
      "authors": [
        "Shayne Longpre",
        "Gregory Yauney",
        "Emily Reif",
        "Katherine Lee",
        "Adam Roberts",
        "Barret Zoph",
        "Denny Zhou",
        "Jason Wei",
        "Kevin Robinson",
        "David Mimno",
        "Daphne Ippolito"
      ],
      "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2305.13169"
    },
    {
      "id": "doremi-2023",
      "name": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
      "type": "paper",
      "tab": "literature",
      "tags": ["Related Work"],
      "shared": false,
      "lastModified": "2025-08-29",
      "description": "Proposes domain reweighting method that optimizes data mixture proportions, improving efficiency and performance across all domains",
      "publicationDate": "May 2023",
      "authors": [
        "Sang Michael Xie",
        "Hieu Pham",
        "Xuanyi Dong",
        "Nan Du",
        "Hanxiao Liu",
        "Yifeng Lu",
        "Percy Liang",
        "Quoc V. Le",
        "Tengyu Ma",
        "Adams Wei Yu"
      ],
      "abstract": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.",
      "readingStatus": "to-read",
      "link": "https://arxiv.org/abs/2305.10429"
    }
  ]
}