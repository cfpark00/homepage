{
  "title": "Brain Evolution, Continual Learning, Website UI, Memory Reconstruction",
  "thoughts": [
    {
      "id": 1,
      "content": "Watched this talk [\"Genes, Cognition, and Human Brain Evolution\"](https://www.youtube.com/watch?v=tN1Bc-WT0oQ) from Dr. Christopher Walsh. I learned about how the human brain evolved in size and what happened in genetic space. It's quite interesting that the brain size is going through an exponential when seen in millions of year scale and yet everything we consider \"highly intelligent\" from agriculture, writing, computers etc is really just a dot on the exponential. So we are in that exponential, trying to create an exponentially improving AI even within that dot! I have also learned about Human Accelerated Regions which seems to be regions in the non-coding part of the genome which are significantly different only in humans compared to other mammals. Given the speed of how slow evolving new genes are, it is really plausible that HARs modifying the expression pattern and neuronal development pattern made all the difference! This sounds really fascinating, but is there any way to reproduce such a phenomena in silico? It must probably start pretty \"forced to happen\" but it would still be cool to find the right analogies in the computational space.",
      "time": "00:01",
      "tags": []
    },
    {
      "id": 2,
      "content": "Launched multiple deep research to see if people have tried to solve continual learning with evolutionary/genetic algorithms",
      "time": "00:05",
      "tags": []
    },
    {
      "id": 3,
      "content": "I long wanted to find the best way to track all my research ideas and thinking process. But maybe its just time to stop finding the right tool or designing the right tool. I should just track it in a very raw form in json. Later, we can get a better system.",
      "time": "00:10",
      "tags": []
    },
    {
      "id": 4,
      "content": "Enhanced thought tracking visual aspect on my website. I also made a password system since it feels weird to reveal literally all thoughts to everyone on the web. But okay, nothing too private will go here, so superficial block seems fine... If they are a dev, sure, steal my thoughts.",
      "time": "00:40",
      "tags": []
    },
    {
      "id": 5,
      "content": "Removed all tags from thoughts. These tags will be generated by AI later anyways and I should make a system which generates adequate tags.",
      "time": "00:50",
      "tags": []
    },
    {
      "id": 6,
      "content": "Okay, let me try to reconstruct Aug 25,26 from episodic memory....",
      "time": "01:00",
      "tags": []
    },
    {
      "id": 7,
      "content": "I should really make a convincing figure of why we should train LLMs with huge amount of ablated concepts. I think i can actually try to visualize what people are even benchmarking using the recent review paper [\"A Survey on Large Language Model Benchmarks\"](https://arxiv.org/abs/2508.15361). I guess i want a figure where we really have models trained on everything and we are benchmarking on their borders. I should propose that if we ablate significant amount of concept-area, we can get truly full ood benchmarks for free.. idk how much this makes sense... lets see...",
      "time": "01:50",
      "tags": []
    },
    {
      "id": 8,
      "content": "The double pendulum demo i made is cool :). I guess obviously there should be a blog post which draws the Lyapunov exponent for all different values of initial conditions.... time to ask claude but maybe tmrw...",
      "time": "01:58",
      "tags": []
    },
    {
      "id": 9,
      "content": "hmm but I'm not going to waste 1+ h every day for though tracking... maybe this should actually be an iphone app, to somehow make it much more comfortable....",
      "time": "02:00",
      "tags": []
    },
    {
      "id": 10,
      "content": "Random twitter scrolling. A bit worried that I have too many ideas but not delivering enough grounded plots.",
      "time": "11:00",
      "tags": []
    },
    {
      "id": 11,
      "content": "Working at Kempner. A bit distracted managing my website's portal page, but I think its actually a good idea to make it good, so that I can organize my research ideas.",
      "time": "13:00",
      "tags": []
    },
    {
      "id": 12,
      "content": "Making this, i wonder myself: Is there really no good platform for research/idea/thought tracking? Am I reinventing the wheel? But my friends doesn't seem to know one... most of them just use a scatter of google docs, phone notes, slack self-messages etc. I mean I'm barely spending any additional time since I put them right in claude code, so maybe its all fine.",
      "time": "13:20",
      "tags": []
    },
    {
      "id": 13,
      "content": "Lets actually read some real papers. The recent paper [\"Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis\"](https://arxiv.org/abs/2505.11581) was on my reading list for a while.",
      "time": "13:40",
      "tags": []
    },
    {
      "id": 14,
      "content": "Wow this paper is already quite good. While honestly I think I shared the intuitions on this paper, they really put down the things in concrete wording. I always seem to undervalue how important it is to concretely put things down. I should learn this. It helps to 1) share ideas and create terms for efficient discussion and 2) even force yourself to define the thoughts more properly.",
      "time": "14:00",
      "tags": []
    },
    {
      "id": 15,
      "content": "I am kinda suspicious what they mean by trained to SGD for a single image, I guess it doesn't literally mean that since people probably know that representations form via grinding through a lot of data, i.e. compression. Should find out.",
      "time": "14:10",
      "tags": []
    },
    {
      "id": 16,
      "content": "Okay reading the paper is fun, but I'm a bit too ADHD to read it all in one go. Unfortunately, I almost cannot not procastinate. Let me constructe my website's portals research management page. I want a nice structuring where each research topic has a list of related papers read.",
      "time": "14:30",
      "tags": []
    },
    {
      "id": 17,
      "content": "Okay seems like claude does it well. How do i stop myself from taking memos in other media compared to here? Maybe I need a easy way to access this like ctrl cv a twitter link here just as i share to my friends/collaborators.",
      "time": "14:40",
      "tags": []
    },
    {
      "id": 18,
      "content": "I feel like maybe AI tool itself is actually the right way for me to organize thoughts. I mean at the end of the day why not? like why should it be a proper GUI, basically claude code and vercel auto deploy is effectively already part of the \"app backend\"",
      "time": "14:50",
      "tags": []
    },
    {
      "id": 19,
      "content": "I should organize this project with name \"Origins of Representations\" basically this is wehre i start studying where do representations come from and can RL create new representations, etc. I feel like this is a major question now. I'm not sure if \"reverse engineering\" circuits is useful but I guess its probably true that good representations are basis of learning and if RL manages to teach LLMs(or any AI) whole new knowledge, it means it must create very good representations.",
      "time": "14:55",
      "tags": []
    },
    {
      "id": 20,
      "content": "Ok for motivation the project needs a good logo. let me try gemini. Wow really cool logo but this watermark kinda sucks. Lets try my paid account. Well seems like the paid version still have watermark. After googling up seems true it always have watermarks.",
      "time": "15:00",
      "tags": []
    },
    {
      "id": 21,
      "content": "Seems like ChatGPT doesn't watermark, but actually gemini visuals are better. kinda interesting. I mean people will really hate watermarks, this is real commitment from google",
      "time": "15:05",
      "tags": []
    },
    {
      "id": 22,
      "content": "Ok just going to crop away the watermark. By the way I need to talk to NS. Where is she?",
      "time": "15:10",
      "tags": []
    },
    {
      "id": 23,
      "content": "Ok long long talk with NS. Nice criticism/critique on my project proposal of ablated LLM training for OOD eval. I think similar criticism as NM, basically about filtering efficiency. That's fair and i still think the research will work since models will suck at whatever large concept missing even thought some rare data pass through (i guess there are papers showing the tails are ignored?) but she said it in a very good summarized way: \"She's pessimistic about data filtering, I'm pessimistic about model's learning efficiency.\" Nice way to put it. I also got some more updates on how much people have scaled up synthetically generated data (not LLM-synthetic data). Basically seems like not so much. Some PCFG work seems to exist though.....",
      "time": "16:45",
      "tags": []
    },
    {
      "id": 24,
      "content": "Discussed IG's research on multilingual LLMs. Mechanistic interpretability seems cool, but currently the bar is higher than a few years ago, usually there needs to be an impact toward the decision making process of future researchers/engineers.",
      "time": "18:30",
      "tags": []
    },
    {
      "id": 25,
      "content": "Had dinner, time to get back into reading the representation paper.",
      "time": "23:30",
      "tags": []
    },
    {
      "id": 26,
      "content": "ended up procastinating and modularizing content vs framework of the projects",
      "time": "23:45",
      "tags": []
    }
  ]
}