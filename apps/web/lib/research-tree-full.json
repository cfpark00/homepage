{
  "researchTree": [
    {
      "id": "1",
      "title": "How does in-context learning work in Transformers? Lets try to understand using synthetically generated data!",
      "type": "question",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 5
      },
      "connections": [
        "2",
        "11"
      ]
    },
    {
      "id": "2",
      "title": "What synthetic data should we use?",
      "type": "question",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 6
      },
      "connections": [
        "66"
      ]
    },
    {
      "id": "3",
      "title": "I guess people used parity prediction, context free grammars, etc. Should we combine multiple of these? Maybe we can see at least task identification and generalization to unseen tasks?",
      "type": "thought",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 8
      },
      "connections": [
        "4",
        "5"
      ]
    },
    {
      "id": "4",
      "title": "Write up multiple data generation processes (DGPs) multiple tasks like: parity, simple cfg, copy, count, etc..",
      "type": "work",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 9
      },
      "connections": [
        "6"
      ]
    },
    {
      "id": "5",
      "title": "Write up codebase to train transformers on different DGPs and evaluate their autoregressive predictive capabilities.",
      "type": "work",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 3,
        "col": 9
      },
      "connections": [
        "6"
      ]
    },
    {
      "id": "6",
      "title": "Train transformer on the dataset and test it on ID and OOD data.",
      "type": "experiment",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 10
      },
      "connections": [
        "7"
      ]
    },
    {
      "id": "7",
      "title": "Kind of as expected transformers perform well ID but not so well OOD.",
      "type": "observation",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 11
      },
      "connections": [
        "8"
      ]
    },
    {
      "id": "8",
      "title": "Hmm all DGPs are quite different and it's simply hard to conclude anything about the relation between the training tasks and the generalization capacity.",
      "type": "thought",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 12
      },
      "connections": [
        "9"
      ]
    },
    {
      "id": "9",
      "title": "I think we should find a dataset where we can clearly control the number of tasks, and quantify the \"distance\" between tasks.",
      "type": "thought",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 2,
        "col": 13
      },
      "connections": [
        "10"
      ]
    },
    {
      "id": "10",
      "title": "Let's pivot directions, clean up and archive this code, but we will reuse parts so refactor to make the interface clean.",
      "type": "pivot",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 2,
        "col": 14
      },
      "connections": [
        "11"
      ]
    },
    {
      "id": "11",
      "title": "What synthetic data allows us to define an arbitrary number of tasks which we can kind of quantify the distance?",
      "type": "question",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 4,
        "col": 6
      },
      "connections": [
        "12"
      ]
    },
    {
      "id": "12",
      "title": "Perhaps Markov chains?",
      "type": "idea",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 4,
        "col": 7
      },
      "connections": [
        "13",
        "14"
      ]
    },
    {
      "id": "13",
      "title": "Set up a data generation process producing data from Markovian transition matrices.",
      "type": "work",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 4,
        "col": 8
      },
      "connections": [
        "16"
      ]
    },
    {
      "id": "14",
      "title": "What distribution do we sample the transition matrices from?",
      "type": "question",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 5,
        "col": 8
      },
      "connections": [
        "15"
      ]
    },
    {
      "id": "15",
      "title": "Just naturally uniform then divide by sum?",
      "type": "thought",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 5,
        "col": 9
      },
      "connections": [
        "16"
      ]
    },
    {
      "id": "16",
      "title": "Set up a dataset with multiple Markov transition matrices from the uniform distribution.",
      "type": "work",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 4,
        "col": 10
      },
      "connections": [
        "17"
      ]
    },
    {
      "id": "17",
      "title": "Train transformers on data generated from a random choice of Markov transition matrix.",
      "type": "experiment",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 6
      },
      "connections": [
        "18"
      ]
    },
    {
      "id": "18",
      "title": "Nice, the loss curve shows a beautiful plateau-bump for the loss! Is this interesting?",
      "type": "observation",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 7
      },
      "connections": [
        "19",
        "26"
      ]
    },
    {
      "id": "19",
      "title": "What is this bump? Is there a mathematical interpretation?",
      "type": "thought",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 8
      },
      "connections": [
        "20"
      ]
    },
    {
      "id": "20",
      "title": "I think the first plateau is just predicting based on the histogram and the lower part is actually predicting using the full matrix?",
      "type": "hypothesis",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 9
      },
      "connections": [
        "21"
      ]
    },
    {
      "id": "21",
      "title": "Implement calculation of expected cross entropy when tokens are output from the histogram versus the full bigram distribution",
      "type": "work",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 10
      },
      "connections": [
        "22"
      ]
    },
    {
      "id": "22",
      "title": "Plot the loss curve together with the theoretical expectations",
      "type": "analysis",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 11
      },
      "connections": [
        "23"
      ]
    },
    {
      "id": "23",
      "title": "It matches\u2026 but not perfectly\u2026",
      "type": "observation",
      "date": "2025-08-02",
      "gridPosition": {
        "row": 6,
        "col": 12
      },
      "connections": [
        "24"
      ]
    },
    {
      "id": "24",
      "title": "Maybe it's because the transformer doesn't perfectly know what matrix the data is coming from. Should we set up a Bayesian analysis to incorporate this?",
      "type": "thought",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 6,
        "col": 13
      },
      "connections": [
        "25"
      ]
    },
    {
      "id": "25",
      "title": "Set up a proper Bayesian calculation of the next token probabilities.",
      "type": "work",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 6,
        "col": 14
      },
      "connections": [
        "29"
      ]
    },
    {
      "id": "26",
      "title": "Oh but on a parallel ground, what happens if we scale up the model? Perhaps some interesting scaling law can be seen? Lets make the codebase such that we can explore this easily?",
      "type": "idea",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 7,
        "col": 8
      },
      "connections": [
        "27"
      ]
    },
    {
      "id": "27",
      "title": "Set up proper config tracking including model and data hyper parameters.",
      "type": "work",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 7,
        "col": 9
      },
      "connections": [
        "28",
        "43"
      ]
    },
    {
      "id": "28",
      "title": "Run experiments varying model width and number of chains.",
      "type": "experiment",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 7,
        "col": 10
      },
      "connections": [
        "67"
      ]
    },
    {
      "id": "29",
      "title": "Back to the Bayesian vs actual results lets plot loss against the Bayesian baselines.",
      "type": "analysis",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 6,
        "col": 15
      },
      "connections": [
        "30"
      ]
    },
    {
      "id": "30",
      "title": "It seems like the loss indeed matches perfectly with the unigram and bigram expectations.",
      "type": "thought",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 6,
        "col": 16
      },
      "connections": [
        "31"
      ]
    },
    {
      "id": "31",
      "title": "The two loss plateaus correspond to the unigram and bigram strategies.",
      "type": "finding",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 6,
        "col": 17
      },
      "connections": [
        "41"
      ]
    },
    {
      "id": "32",
      "title": "The varying model width and number of chains experiments are done. Wow! we see that the second plateau is not reached when the model is too small! It's a very sharp transition at width 128 for state space 10. WOW. Does this explain emergence??",
      "type": "observation",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 9,
        "col": 12
      },
      "connections": [
        "33"
      ]
    },
    {
      "id": "33",
      "title": "Is this a perfect model of emergence? Can we make this claim? What should we check for? A small literature review?",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 13
      },
      "connections": [
        "34"
      ]
    },
    {
      "id": "34",
      "title": "Review scaling laws and emergence.",
      "type": "literature-review",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 14
      },
      "connections": [
        "35"
      ]
    },
    {
      "id": "35",
      "title": "We should probably run them with equal compute and not equal number of steps. How to give them equal compute?",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 15
      },
      "connections": [
        "36"
      ]
    },
    {
      "id": "36",
      "title": "Review transformer flops computation.",
      "type": "literature-review",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 16
      },
      "connections": [
        "37"
      ]
    },
    {
      "id": "37",
      "title": "Okay seems like C=6*D*N will work for us.",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 17
      },
      "connections": [
        "38"
      ]
    },
    {
      "id": "38",
      "title": "Run models of different size with equal-ish compute",
      "type": "experiment",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 9,
        "col": 18
      },
      "connections": [
        "53"
      ]
    },
    {
      "id": "39",
      "title": "Wait actually it seems like there also is a data diversity threshold? Under some data diversity, we don't see the second phase even for big models?",
      "type": "observation",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 7,
        "col": 12
      },
      "connections": [
        "40",
        "45"
      ]
    },
    {
      "id": "40",
      "title": "Things are getting complicated. Let's review what other researchers have done.",
      "type": "literature-review",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 7,
        "col": 13
      },
      "connections": [
        "41"
      ]
    },
    {
      "id": "41",
      "title": "Oh by the way isn't this bump just the one observed in Edelman et al?",
      "type": "question",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 6,
        "col": 18
      },
      "connections": [
        "42"
      ]
    },
    {
      "id": "42",
      "title": "Hmm maybe this bump is the transition from Bayesian retrieval to in context learning?",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 6,
        "col": 19
      },
      "connections": [
        "46"
      ]
    },
    {
      "id": "43",
      "title": "Hmm, maybe I can try a bunch of other stuff like RoPE, no attention, no MLP, thin MLP, different optimizer, different learning rate, different batch size, etc?",
      "type": "thought",
      "date": "2025-08-05",
      "gridPosition": {
        "row": 10,
        "col": 10
      },
      "connections": [
        "44"
      ]
    },
    {
      "id": "44",
      "title": "Run many different experiments with all sorts of varying settings.",
      "type": "experiment",
      "date": "2025-08-05",
      "gridPosition": {
        "row": 10,
        "col": 11
      },
      "connections": [
        "56",
        "58",
        "60",
        "62",
        "64"
      ]
    },
    {
      "id": "45",
      "title": "Hmm a bit confused as the pre-bigram KL divergence doesn't exactly match the expected unigram KL for OOD chains. Also why is the OOD kl growing with more training?",
      "type": "observation",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 13
      },
      "connections": [
        "46"
      ]
    },
    {
      "id": "46",
      "title": "Maybe plotting N context vs KL might help.",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 14
      },
      "connections": [
        "47"
      ]
    },
    {
      "id": "47",
      "title": "Plot N context vs KL.",
      "type": "analysis",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 15
      },
      "connections": [
        "48"
      ]
    },
    {
      "id": "48",
      "title": "Wait what? why does KL grow with more context given when chain is OOD??",
      "type": "analysis",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 16
      },
      "connections": [
        "49"
      ]
    },
    {
      "id": "49",
      "title": "Maybe this is a bug, but lets try to verify with the theoretical Bayesian KL?",
      "type": "thought",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 17
      },
      "connections": [
        "50"
      ]
    },
    {
      "id": "50",
      "title": "Get theoretical values for OOD KL when using a Bayesian Model",
      "type": "experiment",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 18
      },
      "connections": [
        "51"
      ]
    },
    {
      "id": "51",
      "title": "Wait, theoretical KL also grows? How? with more sequence length, aren't we getting better at selecting the closest?",
      "type": "observation",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 19
      },
      "connections": [
        "52"
      ]
    },
    {
      "id": "52",
      "title": "Oh!!! It's because we are doing high dimensional Bayesian inference! In high dimensions, the nearest sample is often further away than the mean of the distribution!!!!!",
      "type": "eureka",
      "date": "2025-08-04",
      "gridPosition": {
        "row": 8,
        "col": 20
      },
      "connections": []
    },
    {
      "id": "53",
      "title": "Plot the KL divergence depending on data diversity and model size again.",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 9,
        "col": 19
      },
      "connections": [
        "54"
      ]
    },
    {
      "id": "54",
      "title": "The emergence isn't showing up any more, everything is smooth",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 9,
        "col": 20
      },
      "connections": [
        "55"
      ]
    },
    {
      "id": "55",
      "title": "Oh!!! so what we thought was emergence was in fact just being unfair to small models by training on the same number of steps!",
      "type": "eureka",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 9,
        "col": 21
      },
      "connections": [
        "63"
      ]
    },
    {
      "id": "56",
      "title": "Overplot all plots color-coded by batch size.",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 11,
        "col": 12
      },
      "connections": [
        "57"
      ]
    },
    {
      "id": "57",
      "title": "Hmm, nothing significant, I guess low batch gives more variance on the training curve",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 11,
        "col": 13
      },
      "connections": [
        "68"
      ]
    },
    {
      "id": "58",
      "title": "Overplot all plots color-coded by learning rate.",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 14,
        "col": 12
      },
      "connections": [
        "59"
      ]
    },
    {
      "id": "59",
      "title": "Hmm, nothing interesting, bigger learning rates learn faster and too big learning rates cannot learn.",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 14,
        "col": 13
      },
      "connections": [
        "69"
      ]
    },
    {
      "id": "60",
      "title": "Compare baseline with thin MLP (ratio=0.25)",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 12,
        "col": 12
      },
      "connections": [
        "61"
      ]
    },
    {
      "id": "61",
      "title": "Hmm, these results suggest that smaller MLP means the OOD generalization comes faster! Makes sense since a large MLP allows memorization? But the trend is significant but weak though, i guess small MLPs can still store a lot of information\u2026.",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 12,
        "col": 13
      },
      "connections": [
        "71"
      ]
    },
    {
      "id": "62",
      "title": "Overplot all plots color-coded by network depth.",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 10,
        "col": 12
      },
      "connections": [
        "63"
      ]
    },
    {
      "id": "63",
      "title": "Interesting. At least depth 2 is needed for the bump. But deeper networks converge faster! But this is probably because bigger models train faster again.",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 10,
        "col": 13
      },
      "connections": [
        "70"
      ]
    },
    {
      "id": "64",
      "title": "Compare learned positional embedding(default) vs RoPE",
      "type": "analysis",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 13,
        "col": 12
      },
      "connections": [
        "65"
      ]
    },
    {
      "id": "65",
      "title": "Oh wow! RoPE makes the plateau way way shorter! I should run everything with RoPE. But it seems like it doesn't affect the final state, so it seems like RoPE is the way to go.",
      "type": "thought",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 13,
        "col": 13
      },
      "connections": []
    },
    {
      "id": "66",
      "title": "Review what kind of synthetic data people used to understand transformers?",
      "type": "literature-review",
      "date": "2025-08-01",
      "gridPosition": {
        "row": 2,
        "col": 7
      },
      "connections": [
        "3"
      ]
    },
    {
      "id": "67",
      "title": "Draw a heatmap of OOD KL divergence depending on data diversity and model size",
      "type": "analysis",
      "date": "2025-08-03",
      "gridPosition": {
        "row": 7,
        "col": 11
      },
      "connections": [
        "32",
        "39"
      ]
    },
    {
      "id": "68",
      "title": "Pause: Batch size effects seem minimal",
      "type": "pause",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 11,
        "col": 14
      },
      "connections": []
    },
    {
      "id": "69",
      "title": "Pause: Learning rate behaves as expected",
      "type": "pause",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 14,
        "col": 14
      },
      "connections": []
    },
    {
      "id": "70",
      "title": "Pause: Depth effect seems as expected",
      "type": "pause",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 10,
        "col": 14
      },
      "connections": []
    },
    {
      "id": "71",
      "title": "Bigger MLPs make memorization easier and slowing generalization !",
      "type": "finding",
      "date": "2025-08-06",
      "gridPosition": {
        "row": 12,
        "col": 14
      },
      "connections": []
    }
  ]
}